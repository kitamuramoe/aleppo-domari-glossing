{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kitamuramoe/aleppo-domari-glossing/blob/main/aleppo_domari_glossing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jQ4Zb80xgWB"
      },
      "source": [
        "## Step 1a: Morphological Segmentation (Preprocessing + Word Replacement)\n",
        "\n",
        "Insert morphological boundaries into words in two steps:\n",
        "\n",
        "1. **Preprocessing**:  \n",
        "   This step adds a space or hyphen (`-`) after specific punctuation marks or endings.  \n",
        "   For example, the clitic `šii` (meaning ‘also’) is separated from the preceding word,  \n",
        "   as it typically follows an inflected phrase and should be treated as an independent morpheme.  \n",
        "   This prevents segmentation errors in the next step.\n",
        "\n",
        "2. **Word replacement using a segmentation list**:  \n",
        "   Words are checked for known endings and replaced with their segmented equivalents.  \n",
        "   These replacements are defined in a text file (`morpheme_boundary_dic.txt`)  \n",
        "   where each line lists an original word ending and its segmented version (with hyphens).  \n",
        "   Longer patterns are applied first to avoid partial matches overriding full ones.\n",
        "\n",
        "Lines starting with a quotation mark (e.g., translations) are skipped and preserved as-is.\n",
        "\n",
        "Example :\n",
        "- `saaʕidkarrisaa` → `saaʕid- -kar-r-is-aa .`\n",
        "\n",
        "## Input Format\n",
        "\n",
        "The input file should consist of unsegmented Aleppo Domari sentences,  \n",
        "each followed by an English translation enclosed in quotation marks (e.g., `'`).  \n",
        "Each sentence pair should appear on two consecutive lines, followed by a blank line to separate entries.  \n",
        "Lines starting with a quotation mark will be preserved throughout all processing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU3L1mEBonUz"
      },
      "outputs": [],
      "source": [
        "def preprocess_morpheme_boundaries(input_file, output_file):\n",
        "    # Define a mapping of patterns to replace\n",
        "    replacements = {\n",
        "        ',' : ' ,',\n",
        "        '.' : ' .',\n",
        "        '!' : ' !',\n",
        "        '?' : ' ?',\n",
        "        ':' : ' :',\n",
        "        ';' : ' ;',\n",
        "        'šii': ' -šii',\n",
        "        '!?' : ' !?'\n",
        "    }\n",
        "\n",
        "    # Read the input file\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Process each line\n",
        "    updated_lines = []\n",
        "    for line in lines:\n",
        "      # Skip lines that begin with a quotation mark\n",
        "        if line.startswith(\"'\"):\n",
        "            updated_lines.append(line.strip())\n",
        "            continue\n",
        "        # Split the line into words and process each word\n",
        "        words = line.strip().split()\n",
        "        updated_words = []\n",
        "        for word in words:\n",
        "            # Check if the word ends with any pattern in the replacements dictionary\n",
        "            for original, replacement in replacements.items():\n",
        "                if word.endswith(original):\n",
        "                    word = word[:len(word) - len(original)] + replacement\n",
        "                    break  # Stop checking other patterns once a match is found\n",
        "            updated_words.append(word)\n",
        "        # Reconstruct the line with updated words\n",
        "        updated_lines.append(' '.join(updated_words))\n",
        "\n",
        "    # Write the updated content to the output file\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        file.write('\\n'.join(updated_lines))\n",
        "\n",
        "    print(f\"Processed file saved as {output_file}\")\n",
        "\n",
        "# Process the uploaded file\n",
        "input_file = \"input_texts/sample_input.txt\"\n",
        "output_file = \"output_texts/morpheme_boundary_base.txt\"\n",
        "preprocess_morpheme_boundaries(input_file, output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3McylOGpa8o"
      },
      "outputs": [],
      "source": [
        "def load_replacements(replacements_file):\n",
        "    \"\"\"Load replacements from a text file into a dictionary.\"\"\"\n",
        "    replacements = {}\n",
        "    with open(replacements_file, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line and '\\t' in line:  # Ensure valid entries\n",
        "                original, replacement = line.split('\\t', 1)\n",
        "                replacements[original] = replacement\n",
        "\n",
        "    # Sort replacements by length (longest first) to prevent partial matches interfering\n",
        "    sorted_replacements = dict(sorted(replacements.items(), key=lambda x: len(x[0]), reverse=True))\n",
        "\n",
        "    return sorted_replacements  # Now the function correctly returns the sorted dictionary\n",
        "\n",
        "def add_morpheme_boundaries(input_file, output_file, replacements_file):\n",
        "    \"\"\"Process the input file, replacing words based on the replacements dictionary.\"\"\"\n",
        "\n",
        "    # Load replacements from file\n",
        "    replacements = load_replacements(replacements_file)\n",
        "\n",
        "    # Read the input file\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Process each line\n",
        "    updated_lines = []\n",
        "    for line in lines:\n",
        "        if line.startswith(\"'\"):  # Skip lines that begin with a quotation mark\n",
        "            updated_lines.append(line.strip())\n",
        "            continue\n",
        "\n",
        "        words = line.strip().split()\n",
        "        updated_words = []\n",
        "        for word in words:\n",
        "            for original, replacement in replacements.items():\n",
        "                if word.endswith(original):\n",
        "                    word = word[:len(word) - len(original)] + replacement\n",
        "                    break  # Stop checking other patterns once a match is found\n",
        "            updated_words.append(word)\n",
        "\n",
        "        updated_lines.append(' '.join(updated_words))\n",
        "\n",
        "    # Write the updated content to the output file\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        file.write('\\n'.join(updated_lines))\n",
        "\n",
        "    print(f\"Processed file saved as {output_file}\")\n",
        "\n",
        "# Define file paths\n",
        "replacements_file = \"dictionaries/morpheme_boundary_dic.txt\"\n",
        "input_file = \"output_texts/morpheme_boundary_base.txt\"\n",
        "output_file = \"output_texts/morpheme_boundary_processed.txt\"\n",
        "\n",
        "# Run the function\n",
        "add_morpheme_boundaries(input_file, output_file, replacements_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr5O_fZhl9Z0"
      },
      "source": [
        "## Step 1b: Correcting Over-segmentation\n",
        "\n",
        "This step applies additional adjustments to morpheme segmentation using a list of replacement patterns.  \n",
        "It is used to correct cases where automatic segmentation may have over-applied boundary rules.\n",
        "\n",
        "The script reads a replacement dictionary (`adjust_morpheme_boundary.txt`),  \n",
        "where each line lists an original word and its adjusted version.\n",
        "\n",
        "Replacements are applied from longest to shortest match to avoid partial overlap issues.\n",
        "\n",
        "Example:\n",
        "- `ḍ -o-m` → `ḍom`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OL0AbOhsmQzk"
      },
      "outputs": [],
      "source": [
        "def load_replacements(replacements_file):\n",
        "    \"\"\"Load gloss replacements from a text file into a dictionary, sorted by length (longest first).\"\"\"\n",
        "    replacements = {}\n",
        "    with open(replacements_file, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line and '\\t' in line:  # Ensure valid entries\n",
        "                original, replacement = line.split('\\t', 1)\n",
        "                replacements[original] = replacement\n",
        "\n",
        "    # Sort replacements by length (longest first) to prevent partial matches interfering\n",
        "    sorted_replacements = dict(sorted(replacements.items(), key=lambda x: len(x[0]), reverse=True))\n",
        "    return sorted_replacements\n",
        "\n",
        "def adjust_morpheme_boundaries(input_file, output_file, replacements):\n",
        "    \"\"\"Read a text file, replace occurrences based on a dictionary, and save the result.\"\"\"\n",
        "    # Read content\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Apply replacements\n",
        "    for old, new in replacements.items():\n",
        "        content = content.replace(old, new)\n",
        "\n",
        "    # Write the updated content to the output file\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "\n",
        "    print(f\"Processed file saved as {output_file}\")\n",
        "\n",
        "# Define file paths\n",
        "replacements_file = \"dictionaries/adjust_morpheme_boundary.txt\"\n",
        "input_file = \"output_texts/morpheme_boundary_processed.txt\"  # Change this to your actual input file\n",
        "output_file = \"output_texts/morpheme_boundary_adjusted.txt\"\n",
        "\n",
        "# Load replacements and process the file\n",
        "replacements = load_replacements(replacements_file)\n",
        "adjust_morpheme_boundaries(input_file, output_file, replacements)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjrQHsqiaB-w"
      },
      "source": [
        "## Step 2: Splitting into One Word Per Line\n",
        "\n",
        "This step converts each sentence into a list of individual words, with one word per line.\n",
        "\n",
        "This step is applied to segmented text.  \n",
        "Lines that start with a quotation mark (e.g., translations or comments) are preserved as they are.\n",
        "\n",
        "After this step, **download the output file and manually check whether each word is correctly segmented** into morphemes.  \n",
        "If over-segmentation or incorrect boundaries are found, correct them manually before proceeding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9te3fOxQaBfl"
      },
      "outputs": [],
      "source": [
        "input_file = 'output_texts/morpheme_boundary_adjusted.txt'\n",
        "output_file = 'output_texts/processed_words.txt'\n",
        "\n",
        "# Open the input and output files\n",
        "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    for line in infile:\n",
        "        # Strip leading/trailing whitespace\n",
        "        line = line.strip()\n",
        "\n",
        "        # Skip processing if the line starts with a quotation mark\n",
        "        if not line or line.startswith(\"'\"):\n",
        "            outfile.write(line + '\\n')\n",
        "        else:\n",
        "            # Split the line into words and write each word on a new line\n",
        "            words = line.split()\n",
        "            for word in words:\n",
        "                outfile.write(word + '\\n')\n",
        "\n",
        "print(f\"Processed text has been saved to '{output_file}'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpewYPw5ZtaY"
      },
      "source": [
        "## Step 3: Interlinear Glossing\n",
        "\n",
        "In this step, each word is matched against a predefined gloss dictionary and assigned a gloss if available.\n",
        "\n",
        "The script reads a dictionary file (`aleppo_domari_dic_updated.txt`) from the `dictionaries/` folder.  \n",
        "Each line in the file lists a word and its corresponding gloss, separated by a tab character.\n",
        "\n",
        "The script then reads the list of segmented words from `output_texts/processed_words.txt`,  \n",
        "and for each word:\n",
        "- If a gloss is found, the word and its gloss are written side by side (tab-separated).\n",
        "- If no gloss is found, the word is written alone to indicate a missing entry.\n",
        "\n",
        "Example:\n",
        "- `ḍom` → `ḍom\tDom/people`\n",
        "\n",
        "\n",
        "The glossed output is saved as `output_texts/output_texts_glossed.txt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld6LlXgNdClK"
      },
      "outputs": [],
      "source": [
        "dictfile = open('dictionaries/aleppo_domari_dic_updated.txt', 'r')\n",
        "dic = {}\n",
        "for line in dictfile:\n",
        "    # Strip trailing whitespace and split the line by tabs\n",
        "    line = line.rstrip().split('\\t')\n",
        "    # Ensure there are at least two elements (key and value)\n",
        "    if len(line) >= 2:\n",
        "        key = line[0]\n",
        "        value = line[1]\n",
        "        # Add the key-value pair to the dictionary\n",
        "        dic[key] = value\n",
        "dictfile.close()\n",
        "print(dic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Sal5WsTrFH4c"
      },
      "outputs": [],
      "source": [
        "txtfile = open('output_texts/processed_words.txt')\n",
        "words = []\n",
        "for i in txtfile:\n",
        "  i = i.rstrip()\n",
        "  words.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "how5ZTGdKJKv"
      },
      "outputs": [],
      "source": [
        "output_file = 'output_texts/glossed.txt'\n",
        "\n",
        "# Open the output file for writing\n",
        "with open(output_file, 'w', encoding='utf-8') as out_f:\n",
        "    # Iterate through each word in the list\n",
        "    for w in words:\n",
        "        if w in dic:\n",
        "            # If the word is in the dictionary, write the word and its value\n",
        "            out_f.write(f\"{w}\\t{dic[w]}\\n\")\n",
        "        else:\n",
        "            # If the word is not in the dictionary, write the word alone\n",
        "            out_f.write(f\"{w}\\n\")\n",
        "\n",
        "# Print to verify the output file content (optional)\n",
        "with open(output_file, 'r', encoding='utf-8') as out_f:\n",
        "    print(out_f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc2M-W-B1KMv"
      },
      "source": [
        "## Step 4: Formatting for Interlinear Glossing\n",
        "\n",
        "This step reformats the glossed text into a standard three-line interlinear glossing format.\n",
        "\n",
        "The input file (`output_texts/glossed.txt`) contains one word per line, optionally followed by a gloss separated by a tab.  \n",
        "Each sentence block is separated by an empty line and may include a free translation enclosed in quotation marks (e.g., `'` or `\"`).\n",
        "\n",
        "For each block, the script outputs:\n",
        "\n",
        "1. A segmented sentence (space-separated)\n",
        "2. A gloss line (aligned with the sentence)\n",
        "3. A free translation line\n",
        "\n",
        "If a gloss is missing:\n",
        "- `*` is inserted for general words\n",
        "- `-*` is inserted if the word begins with a hyphen (e.g., for clitics)\n",
        "\n",
        "After formatting, an additional cleaning step is applied to remove formatting artifacts such as:\n",
        "- Extra spaces before punctuation (e.g., `' , '` → `','`)\n",
        "- Double asterisks (`**`) from unglossed items\n",
        "- Redundant or broken hyphenation (e.g., `- -` → `-`)\n",
        "\n",
        "Example:\n",
        "- ḥasanee laavtiiy-ee,\n",
        "- `*` girl-COP.PRES/2SG.PRES/PL\n",
        "- 'the daughter of ḥasanee,'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjxmPnyHT1TS"
      },
      "outputs": [],
      "source": [
        "# Function to process the input file and reformat the content\n",
        "def format_glossing(input_file, output_file):\n",
        "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        sentence = []\n",
        "        gloss = []\n",
        "        translation = None\n",
        "\n",
        "        for line in infile:\n",
        "            line = line.strip()\n",
        "            if not line:  # Empty line indicates end of an entry\n",
        "                if sentence and gloss and translation:\n",
        "                    outfile.write(' '.join(sentence) + '\\n')\n",
        "                    outfile.write(' '.join(gloss) + '\\n')\n",
        "                    outfile.write(translation + '\\n\\n')\n",
        "                sentence = []\n",
        "                gloss = []\n",
        "                translation = None\n",
        "                continue\n",
        "\n",
        "            # Detect English translation enclosed in single or double quotes\n",
        "            if line.startswith((\"'\", '\"', \"‘\", \"’\")) and line.endswith((\"'\", '\"', \"‘\", \"’\")):\n",
        "                translation = line\n",
        "            else:\n",
        "                # Extract the word and gloss\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) == 2:\n",
        "                    sentence.append(parts[0])\n",
        "                    gloss.append(parts[1])\n",
        "                elif len(parts) == 1:  # If no gloss is provided\n",
        "                    sentence.append(parts[0])\n",
        "                    if parts[0].startswith('-'):\n",
        "                        gloss.append('-*')  # Add '-*' if the sentence part starts with '-'\n",
        "                    else:\n",
        "                        gloss.append('*')   # Default: append '*'\n",
        "\n",
        "        # Write the last entry if the file doesn't end with a blank line\n",
        "        if sentence and gloss and translation:\n",
        "            outfile.write(' '.join(sentence) + '\\n')\n",
        "            outfile.write(' '.join(gloss) + '\\n')\n",
        "            outfile.write(translation + '\\n')\n",
        "\n",
        "# Specify input and output file paths\n",
        "input_file = 'output_texts/glossed.txt'  # Replace with your actual file name\n",
        "output_file = 'output_texts/formatted_glossing.txt'\n",
        "\n",
        "# Process the file\n",
        "format_glossing(input_file, output_file)\n",
        "\n",
        "print(\"Data reformatted and saved to\", output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-FKwy4VFFcF"
      },
      "outputs": [],
      "source": [
        "# Define replacements\n",
        "replacements = {\n",
        "    ' ,': ',',\n",
        "    ' .': '.',\n",
        "    ' !': '!',\n",
        "    ' ?': '?',\n",
        "    ' :': ':',\n",
        "    ' ;': ';',\n",
        "    ' **': '',\n",
        "    '- -': '-',\n",
        "    ' -': '-',\n",
        "    '- ': '-'\n",
        "}\n",
        "\n",
        "# Load the file\n",
        "input_file = \"output_texts/formatted_glossing.txt\"  # Adjust path if necessary\n",
        "output_file = \"output_texts/cleaned_formatted_glossing.txt\"\n",
        "\n",
        "# Read content\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Apply replacements\n",
        "for old, new in replacements.items():\n",
        "    content = content.replace(old, new)\n",
        "\n",
        "# Save cleaned content\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(content)\n",
        "\n",
        "print(f\"Cleaning complete. Saved as {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4Qwy3EpfBib"
      },
      "source": [
        "## Step 5: Gloss Disambiguation\n",
        "\n",
        "This step performs **gloss disambiguation**—it replaces multiple gloss entries with more specific or context-appropriate ones.\n",
        "\n",
        "The input file (`output_texts/cleaned_formatted_glossing.txt`) may contain glosses with multiple possible analyses.  \n",
        "To improve clarity, a set of replacement rules is applied using a dictionary file (`gloss_replacement.txt`), where each line lists an original gloss string and its disambiguated version, separated by a tab.\n",
        "\n",
        "Example:\n",
        "- d-išt-ir-r-ee\n",
        "- give-COP/PROG-3SG-2SG-PRES → give-PROG-3SG-2SG-PRES\n",
        "\n",
        "The disambiguated and finalized output is saved as `output_texts/formatted_glossing_final.txt`.\n",
        "\n",
        "This automatic disambiguation process works for glosses that can be clarified based on surrounding morphemes.  \n",
        "However, certain glosses—such as those involving lexemes with multiple context-dependent meanings (e.g., `qay-` glossed as `eat/vomit`)—cannot be resolved by this process.  \n",
        "Such cases should be manually reviewed and resolved after this step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09p6_yYSio4m"
      },
      "outputs": [],
      "source": [
        "def load_replacements(replacements_file):\n",
        "    \"\"\"Load gloss replacements from a text file into a dictionary, sorted by length (longest first).\"\"\"\n",
        "    replacements = {}\n",
        "    with open(replacements_file, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line and '\\t' in line:  # Ensure valid entries\n",
        "                original, replacement = line.split('\\t', 1)\n",
        "                replacements[original] = replacement\n",
        "\n",
        "    # Sort replacements by length (longest first) to prevent partial matches interfering\n",
        "    sorted_replacements = dict(sorted(replacements.items(), key=lambda x: len(x[0]), reverse=True))\n",
        "    return sorted_replacements\n",
        "\n",
        "def replace_text(input_file, output_file, replacements):\n",
        "    \"\"\"Read a text file, replace occurrences based on a dictionary, and save the result.\"\"\"\n",
        "    # Read content\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Apply replacements\n",
        "    for old, new in replacements.items():\n",
        "        content = content.replace(old, new)\n",
        "\n",
        "    # Write the updated content to the output file\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "\n",
        "    print(f\"Processed file saved as {output_file}\")\n",
        "\n",
        "# Define file paths\n",
        "replacements_file = \"dictionaries/gloss_replacement.txt\"\n",
        "input_file = \"output_texts/cleaned_formatted_glossing.txt\"  # Change this to your actual input file\n",
        "output_file = \"output_texts/formatted_glossing_final.txt\"\n",
        "\n",
        "# Load replacements and process the file\n",
        "replacements = load_replacements(replacements_file)\n",
        "replace_text(input_file, output_file, replacements)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPnD/XNkvCMcc4kW5qvVXAK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
